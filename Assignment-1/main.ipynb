{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import contextualSpellCheck\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm, trange\n",
    "from utils import *\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def save(filename, *args):\n",
    "    # Get global dictionary\n",
    "    glob = globals()\n",
    "    d = {}\n",
    "    for v in args:\n",
    "        # Copy over desired values\n",
    "        d[v] = glob[v]\n",
    "    with open(filename, 'wb') as f:\n",
    "        # Put them in the file\n",
    "        pickle.dump(d, f)\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    # Get global dictionary\n",
    "    glob = globals()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for k, v in pickle.load(f).items():\n",
    "            # Set each global variable to the value from the file\n",
    "            glob[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "contextualSpellCheck.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the docs and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7095eb7c620242e5ba227351c6299906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4f58da9e134a18b382cf181fa14775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1573526ea141f3a4d773c3ae0398da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35eff98bfcb44c2a293ba42baa0482b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : What is Atal Pension Yojana What are its benefits\n",
      "Corrected: What is the Pension? What are its benefits\n",
      "\n",
      "Original : Where is starch digested How is it digested\n",
      "Corrected: Where is it eaten How is it here\n",
      "\n",
      "Original : How do introverts enjoy life\n",
      "Corrected: How do they enjoy life\n",
      "\n",
      "Original : Kindly tell me whole process of admission at vits Vellore for biotechi m a bio student in 12I dont have math there\n",
      "Corrected: Kindly tell me whole process of admission at the Vellore for i m a bio student in I donot have math there\n",
      "\n",
      "Original : How does Quora look to a moderator\n",
      "Corrected: How does Quora look to a man\n",
      "\n",
      "Original : Why does phase shift take place in the output of the common emitter amplifier when compared to the input signal\n",
      "Corrected: Why does phase shift take place in the output of the common linear amplifier when compared to the input signal\n",
      "\n",
      "Original : Why do people say Dhanush South Indian actor is ugly I dont think so\n",
      "Corrected: Why do people say Dhanush South Indian actor is ugly I donot think so\n",
      "\n",
      "Original : How do I reset my Gmail password when I dont remember my recovery information\n",
      "Corrected: How do I get my Gmail password when I donot remember my recovery information\n",
      "\n",
      "Original : How can I get rid of cellulite on my stomach\n",
      "Corrected: How can I get rid of it on my stomach\n",
      "\n",
      "Original : How do I rid myself of my paranoia\n",
      "Corrected: How do I rid myself of my past\n",
      "\n",
      "Original : Will the value of Indian rupee increase after the ban of 500 and 1000 rupee notes\n",
      "Corrected: Will the value of one coins increase after the ban of 500 and 1000 paper notes\n",
      "\n",
      "Original : How does the Laravel authentication work\n",
      "Corrected: How does the actual authentication work\n",
      "\n",
      "Original : What if humans were nocturnal\n",
      "Corrected: What if humans were what\n",
      "\n",
      "Original : How and where did feudalism develop\n",
      "Corrected: How and where did this develop\n",
      "\n",
      "Original : What is the best way to memorize notes\n",
      "Corrected: What is the best way to write notes\n",
      "\n",
      "Original : Is Syrio Forel a faceless man following Arya around being different people\n",
      "Corrected: Is Syrio Forel a wise man following you around being different people\n",
      "\n",
      "Original : What are the effects of demonitization of 500 and 1000 rupees notes on real estate sector\n",
      "Corrected: What are the effects of inflation of 500 and 1000 paper notes on real estate sector\n",
      "\n",
      "Original : How can I get free gems in Clash of Clans\n",
      "Corrected: How can I get free time in Clash of Clans\n",
      "\n",
      "Original : In the world we are second largest country in terms of population but we win only very few medals in the Olympics Why\n",
      "Corrected: In the world we are second largest country in terms of population but we win only very few medals in the Why Why\n",
      "\n",
      "Original : How do I use Facebook in China\n",
      "Corrected: How do I use them in China\n",
      "\n",
      "Original : How do you feel when someone upvotes your answer on Quora\n",
      "Corrected: How do you feel when someone gives your answer on Quora\n",
      "\n",
      "Original : How do I impress the bosss boss\n",
      "Corrected: How do I impress the big boss\n",
      "\n",
      "Original : Are there any other republicans here who voted for Hillary Clinton\n",
      "Corrected: Are there any other Americans here who voted for Hillary Clinton\n",
      "\n",
      "Original : How do I get rid of mosquitoes bites quickly\n",
      "Corrected: How do I get rid of the bites quickly\n",
      "\n",
      "Original : Who are some famous people with low IQs\n",
      "Corrected: Who are some famous people with lows\n",
      "\n",
      "Original : Why should one hate Shahrukh Khan\n",
      "Corrected: Why should one hate Shahrukh and\n",
      "\n",
      "Original : Which are the best and cheapest universities in the USA for doing an MS in computer science\n",
      "Corrected: Which are the best and best universities in the USA for doing an MA in computer science\n",
      "\n",
      "Original : Which microcontrollers are generally used in the robotics industry\n",
      "Corrected: Which microcontrollers are generally used in the automotive industry\n",
      "\n",
      "Original : Can we map the surface and the subsurface of a planet using cosmic rays or very high frequency gamma rays with ground based telescopes\n",
      "Corrected: Can we map the surface and the surface of a planet using cosmic rays or very high frequency gamma rays with ground based methods\n",
      "\n",
      "Original : How do you remove epoxy paint from a concrete surface\n",
      "Corrected: How do you remove your paint from a concrete surface\n",
      "\n",
      "Original : What is so special about Rolls Royce cars that they are only given to respected personalities\n",
      "Corrected: What is so special about Rolls Royce cars that they are only given to respected personalities\n",
      "\n",
      "Original : Does hypnosis work\n",
      "Corrected: Does his work\n",
      "\n",
      "Original : What is Fiverr and how do they make money\n",
      "Corrected: What is life and how do they make money\n",
      "\n",
      "Original : What are the requirements of modulation\n",
      "Corrected: What are the requirements of that\n",
      "\n",
      "Original : Why did Ecuador cut Julian Assanges Internet access\n",
      "Corrected: Why did Ecuador cut Julianian Internet access\n",
      "\n",
      "Original : Did Ben Affleck shine more than Christian Bale as Batman\n",
      "Corrected: Did Ben Affleck shine more than Christian Paul as?\n",
      "\n",
      "Original : Do Trump voters care that he has reneged all his campaign promises\n",
      "Corrected: Do your voters care that he has defeated all his campaign promises\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('cache/docs.pkl') and os.path.exists('cache/queries.pkl'):\n",
    "    load('cache/docs.pkl')\n",
    "    load('cache/queries.pkl')\n",
    "else:\n",
    "    # skip first column\n",
    "    docs = pd.read_csv('Query_Doc/docs.csv', index_col=0)\n",
    "    queries = pd.read_csv('Query_Doc/queries.csv', index_col=0)\n",
    "\n",
    "    # Include another column in the queries data frame containing a list of document ids relevant\n",
    "    # That list is present in a csv file named qdrel.csv\n",
    "    # Headers: query_id, doc_id\n",
    "    qdrel = pd.read_csv('Query_Doc/qdrel.csv', index_col=0)\n",
    "    qdrel = qdrel.groupby('query_id')['doc_id'] \\\n",
    "        .apply(list).reset_index(name='relevant_docs')\n",
    "    queries = queries.merge(qdrel, on='query_id')\n",
    "\n",
    "    # Preprocess the text\n",
    "    docs['doc_text'] = docs['doc_text'].progress_apply(preprocess)\n",
    "    queries['query_text'] = queries['query_text'].progress_apply(preprocess)\n",
    "    \n",
    "    docs['doc_text'] = [correct_spellings(doc, False)\n",
    "                        for doc in nlp.pipe(tqdm(docs['doc_text']))]\n",
    "    queries['query_text'] = [correct_spellings(query, True)\n",
    "                             for query in nlp.pipe(tqdm(queries['query_text']))]\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    os.makedirs('cache', exist_ok=True)\n",
    "\n",
    "    save('cache/docs.pkl', 'docs')\n",
    "    save('cache/queries.pkl', 'queries')\n",
    "# END OF PREPROCESSING7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs   : (10000, 2)\n",
      "Queries: (100, 3)\n",
      "   doc_id                                           doc_text\n",
      "0       1  What is the step by step guide to invest in sh...\n",
      "1       2  What is the step by step guide to invest in sh...\n",
      "2       3     What is the story of Kohinoor KohiNoor Diamond\n",
      "3       4  What would happen if the Indian government sto...\n",
      "4       5  How can I increase the speed of my internet co...\n",
      "   query_id                                         query_text relevant_docs\n",
      "0      4584                 How can ask questions using photos        [4583]\n",
      "1      6588         What is the Pension? What are its benefits        [6587]\n",
      "2     10113                   Where is it eaten How is it here       [10114]\n",
      "3      7957        What is a conjecture What are some examples        [7956]\n",
      "4      5498  What can India do to support the people suffer...        [5497]\n",
      "\n",
      "Index(['doc_id', 'doc_text'], dtype='object')\n",
      "Index(['query_id', 'query_text', 'relevant_docs'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Docs   : {docs.shape}\")\n",
    "print(f\"Queries: {queries.shape}\")\n",
    "\n",
    "print(docs.head())\n",
    "print(queries.head())\n",
    "print()\n",
    "\n",
    "# print the labels\n",
    "print(docs.columns)\n",
    "print(queries.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('contextual spellchecker',\n",
       " <contextualSpellCheck.contextualSpellCheck.ContextualSpellCheck at 0x211f0c83850>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the spell checker from the pipeline\n",
    "nlp.remove_pipe('contextual spellchecker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6de27cc2df6437c9ef4c06f928d66bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f78c650dda4e138c1384d4442d008a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size  : 2273\n",
      "Precision@1 : 0.5400\n",
      "Precision@5 : 0.7700\n",
      "Precision@10: 0.8155\n"
     ]
    }
   ],
   "source": [
    "doc_tokens = [' '.join(token.text for token in doc)\n",
    "              for doc in nlp.pipe(tqdm(docs['doc_text']))]\n",
    "\n",
    "query_tokens = [' '.join(token.text for token in query)\n",
    "                for query in nlp.pipe(tqdm(queries['query_text']))]\n",
    "\n",
    "docVectors, queryVectors, vocab = get_vectors(\n",
    "    doc_tokens, query_tokens)\n",
    "print(\"Vocab size  :\", len(vocab))\n",
    "print_scores(docs, queries, docVectors, queryVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ae2ed3e5204190adeedaeca687ee23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1129da1486d748c5a6efeb239c58c681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size  : 2011\n",
      "Precision@1 : 0.6200\n",
      "Precision@5 : 0.8005\n",
      "Precision@10: 0.8415\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "\n",
    "doc_stems = [' '.join(stemmer.stem(token.text) for token in doc)\n",
    "             for doc in nlp.pipe(tqdm(docs['doc_text']))]\n",
    "\n",
    "query_stems = [' '.join(stemmer.stem(token.text) for token in query)\n",
    "               for query in nlp.pipe(tqdm(queries['query_text']))]\n",
    "\n",
    "docVectors, queryVectors, vocab = get_vectors(\n",
    "    doc_stems, query_stems)\n",
    "print(\"Vocab size  :\", len(vocab))\n",
    "print_scores(docs, queries, docVectors, queryVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1344724b7ed1427687ee017b013aba92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f35c27ad214967a277210cb5ac67f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size  : 1997\n",
      "Precision@1 : 0.6400\n",
      "Precision@5 : 0.7955\n",
      "Precision@10: 0.8328\n"
     ]
    }
   ],
   "source": [
    "doc_lemmas = [' '.join(token.lemma_ for token in doc)\n",
    "              for doc in nlp.pipe(tqdm(docs['doc_text']))]\n",
    "\n",
    "query_lemmas = [' '.join(token.lemma_ for token in query)\n",
    "                for query in nlp.pipe(tqdm(queries['query_text']))]\n",
    "\n",
    "docVectors, queryVectors, vocab = get_vectors(\n",
    "    doc_lemmas, query_lemmas)\n",
    "print(\"Vocab size  :\", len(vocab))\n",
    "print_scores(docs, queries, docVectors, queryVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe1ffc027cb4ad6a3b2b82bdfa1386a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad7857b6fbe4fb196e1c3c1103648cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71631eab25b4a10bcbd4ef1da54b335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5067a2c34214499b183966b4d42859d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size  : 2204\n",
      "Precision@1 : 0.5500\n",
      "Precision@5 : 0.7980\n",
      "Precision@10: 0.8215\n"
     ]
    }
   ],
   "source": [
    "doc_ner_pos = [get_ner_pos(doc)\n",
    "               for doc in nlp.pipe(tqdm(docs['doc_text']))]\n",
    "\n",
    "query_ner_pos = [get_ner_pos(query)\n",
    "                 for query in nlp.pipe(tqdm(queries['query_text']))]\n",
    "\n",
    "docVectors, queryVectors, vocab = get_vectors(\n",
    "    doc_ner_pos, query_ner_pos)\n",
    "\n",
    "noun_idx = []\n",
    "named_idx = []\n",
    "\n",
    "for token, idx in vocab.items():\n",
    "    if token.endswith('+NAMED_ENTITY') or token.endswith('+PROPN'):\n",
    "        named_idx.append(idx)\n",
    "    elif token.endswith('+NOUN'):\n",
    "        noun_idx.append(idx)\n",
    "# END for token\n",
    "\n",
    "# postprocess the vectors [multiply noun by 2, named entity by 4]\n",
    "for i in trange(docVectors.shape[0]):\n",
    "    for j in named_idx:\n",
    "        docVectors[i, j] *= 4\n",
    "    for j in noun_idx:\n",
    "        docVectors[i, j] *= 2\n",
    "# END for i\n",
    "\n",
    "for i in trange(queryVectors.shape[0]):\n",
    "    for j in named_idx:\n",
    "        queryVectors[i, j] *= 4\n",
    "    for j in noun_idx:\n",
    "        queryVectors[i, j] *= 2\n",
    "# END for i\n",
    "\n",
    "print(\"Vocab size  :\", len(vocab))\n",
    "print_scores(docs, queries, docVectors, queryVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7109ae5f12f34547a70345bde082d977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e86080a01b440891982ffbe1313978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "doc_special = [' '.join(stemmer.stem(token.lemma_)\n",
    "                        for token in doc)\n",
    "               for doc in nlp.pipe(tqdm(docs['doc_text']))]\n",
    "\n",
    "query_special = [' '.join(stemmer.stem(token.lemma_)\n",
    "                          for token in query)\n",
    "                 for query in nlp.pipe(tqdm(queries['query_text']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7abbcd1ddec4908989075d86b0ba614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62    0.8055  0.84275]\n",
      "[0.59    0.8225  0.84275]\n",
      "[0.61    0.8225  0.84375]\n",
      "[0.6    0.8105 0.8515]\n",
      "[0.61    0.8305  0.84275]\n",
      "[0.62   0.8135 0.8415]\n",
      "[0.59    0.8205  0.84375]\n",
      "[0.63    0.8285  0.84275]\n",
      "[0.62    0.8105  0.86275]\n",
      "[0.62    0.8135  0.84275]\n",
      "[0.64    0.8075  0.84275]\n",
      "[0.63   0.8135 0.8515]\n",
      "[0.61    0.8255  0.84275]\n",
      "[0.61   0.8205 0.8415]\n",
      "[0.61    0.8105  0.85275]\n",
      "[0.61    0.8125  0.84275]\n",
      "[0.61    0.8105  0.84275]\n",
      "[0.6    0.8085 0.8515]\n",
      "[0.61    0.8385  0.85275]\n",
      "[0.6     0.8305  0.85275]\n",
      "[0.63    0.8185  0.84275]\n",
      "[0.62   0.8235 0.8515]\n",
      "[0.6     0.8005  0.84275]\n",
      "[0.61    0.8305  0.85275]\n",
      "[0.62    0.8255  0.85275]\n",
      "[0.6     0.8185  0.85275]\n",
      "[0.61    0.8085  0.85275]\n",
      "[0.6    0.8325 0.8515]\n",
      "[0.61    0.8175  0.84275]\n",
      "[0.59    0.8155  0.85275]\n",
      "[0.6     0.8105  0.85275]\n",
      "[0.58    0.8185  0.84375]\n",
      "[0.6     0.8235  0.85275]\n",
      "[0.61    0.8305  0.86275]\n",
      "[0.6     0.8305  0.85275]\n",
      "[0.64    0.8185  0.85275]\n",
      "[0.63    0.8205  0.85275]\n",
      "[0.63    0.8285  0.84275]\n",
      "[0.59    0.8205  0.84275]\n",
      "[0.62    0.8185  0.86275]\n",
      "[0.6     0.8235  0.84275]\n",
      "[0.62    0.7985  0.84275]\n",
      "[0.6     0.8255  0.85275]\n",
      "[0.6     0.8185  0.84275]\n",
      "[0.61   0.8225 0.8515]\n",
      "[0.61    0.8205  0.84275]\n",
      "[0.61   0.8205 0.8525]\n",
      "[0.64    0.8175  0.85275]\n",
      "[0.64    0.8245  0.85275]\n",
      "[0.62    0.8155  0.86275]\n",
      "[0.59    0.8255  0.85275]\n",
      "[0.6    0.8105 0.8415]\n",
      "[0.61    0.8205  0.84275]\n",
      "[0.63    0.8205  0.86275]\n",
      "[0.6     0.8305  0.84275]\n",
      "[0.62   0.8105 0.8515]\n",
      "[0.59   0.8205 0.8615]\n",
      "[0.58    0.8205  0.86275]\n",
      "[0.64    0.8375  0.85275]\n",
      "[0.6     0.8125  0.84275]\n",
      "[0.61    0.8125  0.84775]\n",
      "[0.64    0.8205  0.84275]\n",
      "[0.6     0.8105  0.84275]\n",
      "[0.6     0.8105  0.86775]\n",
      "[0.62    0.8205  0.84275]\n",
      "[0.63    0.8075  0.84275]\n",
      "[0.62   0.8185 0.8415]\n",
      "[0.62    0.8175  0.85375]\n",
      "[0.63    0.8075  0.85275]\n",
      "[0.62    0.8005  0.85275]\n",
      "[0.6     0.8085  0.85275]\n",
      "[0.63    0.8225  0.85275]\n",
      "[0.62   0.8235 0.8515]\n",
      "[0.61    0.8225  0.84275]\n",
      "[0.63   0.8285 0.8615]\n",
      "[0.61    0.8055  0.84275]\n",
      "[0.61    0.8305  0.84275]\n",
      "[0.61    0.8105  0.84275]\n",
      "[0.6     0.8035  0.84275]\n",
      "[0.58    0.7955  0.84275]\n",
      "[0.6    0.8205 0.8525]\n",
      "[0.58    0.8185  0.84275]\n",
      "[0.61   0.8405 0.8515]\n",
      "[0.59    0.8185  0.85275]\n",
      "[0.62   0.8305 0.8415]\n",
      "[0.61    0.8355  0.85275]\n",
      "[0.58    0.8055  0.84275]\n",
      "[0.62    0.7985  0.86275]\n",
      "[0.61   0.8285 0.8605]\n",
      "[0.62    0.8035  0.85275]\n",
      "[0.6     0.8285  0.85375]\n",
      "[0.63    0.8155  0.85275]\n",
      "[0.61    0.8225  0.85275]\n",
      "[0.64    0.8075  0.84175]\n",
      "[0.61    0.8305  0.85375]\n",
      "[0.6     0.8205  0.84275]\n",
      "[0.59    0.8205  0.85275]\n",
      "[0.61    0.8005  0.85275]\n",
      "[0.6    0.8235 0.8415]\n",
      "[0.63    0.8205  0.84275]\n",
      "Mean Vocab Size   : 1951.0\n",
      "Mean Precision@1  : 0.6109\n",
      "Mean Precision@5  : 0.8183\n",
      "Mean Precision@10 : 0.8492\n"
     ]
    }
   ],
   "source": [
    "NUM_ITERS = 100\n",
    "RANDOM_SEED = 42\n",
    "vocabs = np.zeros(NUM_ITERS)\n",
    "precisions = np.zeros((NUM_ITERS, 3))\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "for i in trange(NUM_ITERS):\n",
    "    # Add Gaussian Noise\n",
    "    docVectors, queryVectors, vocab = get_vectors(\n",
    "        doc_special, query_special, add_noise=True)\n",
    "\n",
    "    vocabs[i] = len(vocab)\n",
    "    cosine_similarities = cosine_similarity(queryVectors, docVectors)\n",
    "\n",
    "    for j, k in enumerate([1, 5, 10]):\n",
    "        precisions[i, j] = calculate_precision_at_k(docs, queries,\n",
    "                                                    cosine_similarities, k)\n",
    "    # END for j, k\n",
    "\n",
    "    print(precisions[i])\n",
    "# END for i\n",
    "\n",
    "print(f\"Mean Vocab Size   : {vocabs.mean():.1f}\")\n",
    "print(f\"Mean Precision@1  : {precisions[:, 0].mean():.4f}\")\n",
    "print(f\"Mean Precision@5  : {precisions[:, 1].mean():.4f}\")\n",
    "print(f\"Mean Precision@10 : {precisions[:, 2].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
