{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, f1_score,\n",
    "                             precision_score, recall_score)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from viterbi_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = r\"./data/train.txt\"\n",
    "TEST_PATH = r\"./data/test.txt\"\n",
    "\n",
    "train_loader = Loader(TRAIN_PATH)\n",
    "test_loader = Loader(TEST_PATH)\n",
    "calculate_all_counts(train_loader)\n",
    "print(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence: Sentence) -> List[str]:\n",
    "    # Get the words from the sentence\n",
    "    words = [word.word for word in sentence]\n",
    "    # words = [word.norm_word for word in sentence]\n",
    "\n",
    "    # Number of words in the sentence\n",
    "    n = len(sentence)\n",
    "\n",
    "    # Create a table to store the best scores and back pointers\n",
    "    # Each cell stores a dictionary that maps a state to the best score and back pointer\n",
    "    # [word_index] -> {state: (score, back_pointer)}\n",
    "    table: List[Dict[str, Tuple[float, Optional[str]]]] = [{}\n",
    "                                                           for _ in range(n)]\n",
    "\n",
    "    # Initialize the first column of the table\n",
    "    for state in all_states:\n",
    "        table[0][state] = (get_emission_probability(words[0], state) +\n",
    "                           get_start_probability(state),\n",
    "                           None)\n",
    "    # END for state in all_states\n",
    "\n",
    "    # Fill in the rest of the table\n",
    "    for i in range(1, n):\n",
    "        for state in all_states:\n",
    "            best_score = float('inf')\n",
    "            back_pointer = None\n",
    "\n",
    "            for prev_state in all_states:\n",
    "                score = (table[i - 1][prev_state][0] +\n",
    "                         get_transition_probability(prev_state, state) +\n",
    "                         get_emission_probability(words[i], state))\n",
    "\n",
    "                # print(f\"{words[i] = :10} {score = :0.5f} {best_score = :0.5f} \"\n",
    "                #       f\"{prev_state = :5} {state = :5} {back_pointer = }\")\n",
    "\n",
    "                if score < best_score:  # less than because -ve log probs\n",
    "                    best_score = score\n",
    "                    back_pointer = prev_state\n",
    "                # END if score < best_score\n",
    "            # END for prev_state in all_states\n",
    "\n",
    "            table[i][state] = (best_score, back_pointer)\n",
    "        # END for state in all_states\n",
    "    # END for i in range(1, n)\n",
    "\n",
    "    # pp(table)\n",
    "\n",
    "    # Find the best last state\n",
    "    best_last_state = min(table[-1], key=lambda k: table[-1][k][0])\n",
    "\n",
    "    # print(f\"{best_last_state = }\")\n",
    "\n",
    "    # Follow the back pointers to find the best path\n",
    "    best_path: List[str] = []\n",
    "    back_pointer = best_last_state\n",
    "    i = n - 1\n",
    "\n",
    "    while back_pointer is not None and i >= 0:\n",
    "        best_path.append(back_pointer)\n",
    "        back_pointer = table[i][back_pointer][1]\n",
    "        i -= 1\n",
    "\n",
    "    # Reverse the best path\n",
    "    best_path.reverse()\n",
    "\n",
    "    return best_path\n",
    "# END viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(\n",
    "    loader: Loader,\n",
    "    predictions: List[List[str]],\n",
    "    filename: str\n",
    ") -> None:\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        for sentence, tags in zip(loader, predictions):\n",
    "            for word, tag in zip(sentence, tags):\n",
    "                file.write(f\"{sentence.sent_id}\\t\"\n",
    "                           f\"{word.word_index}\\t\"\n",
    "                           f\"{word.word}\\t\"\n",
    "                           f\"{tag}\\t\"\n",
    "                           f\"{word.pos_tag}\\n\")\n",
    "            # END for word, tag in zip(sentence, tags)\n",
    "        # END for sentence, tags in zip(sentences, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = train_loader[500]\n",
    "pred = viterbi(sentence)\n",
    "actual = [word.pos_tag for word in sentence]\n",
    "\n",
    "print(sentence.text)\n",
    "print(pred, len(pred))\n",
    "print(actual, len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on train data\n",
    "predicted_tags: List[List[str]] = []\n",
    "\n",
    "print(\"Train data:\")\n",
    "for sentence in tqdm(train_loader, desc=\"Train data\"):\n",
    "    predicted_tags.append(viterbi(sentence))\n",
    "# END for sentence in train_loader\n",
    "\n",
    "save_predictions(train_loader, predicted_tags, r\"viterbi_predictions_train.tsv\")\n",
    "\n",
    "_predicted_tags = sum(predicted_tags, [])\n",
    "actual_tags = sum(([word.pos_tag for word in sentence]\n",
    "                   for sentence in train_loader), [])\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(actual_tags,\n",
    "                               _predicted_tags),\n",
    "    \"recall\": recall_score(actual_tags,\n",
    "                           _predicted_tags,\n",
    "                           average=\"macro\",\n",
    "                           zero_division=0),\n",
    "    \"precision\": precision_score(actual_tags,\n",
    "                                 _predicted_tags,\n",
    "                                 average=\"macro\",\n",
    "                                 zero_division=0),\n",
    "    \"f1\": f1_score(actual_tags,\n",
    "                   _predicted_tags,\n",
    "                   average=\"macro\",\n",
    "                   zero_division=0)\n",
    "}\n",
    "\n",
    "pp(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test data\n",
    "predicted_tags: List[List[str]] = []\n",
    "\n",
    "print(\"Test data:\")\n",
    "for sentence in tqdm(test_loader, desc=\"Test data\"):\n",
    "    predicted_tags.append(viterbi(sentence))\n",
    "# END for sentence in test_loader\n",
    "\n",
    "save_predictions(test_loader, predicted_tags, r\"viterbi_predictions_test.tsv\")\n",
    "\n",
    "_predicted_tags = sum(predicted_tags, [])\n",
    "actual_tags = sum(([word.pos_tag for word in sentence]\n",
    "                   for sentence in test_loader), [])\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(actual_tags,\n",
    "                               _predicted_tags),\n",
    "    \"recall\": recall_score(actual_tags,\n",
    "                           _predicted_tags,\n",
    "                           average=\"macro\",\n",
    "                           zero_division=0),\n",
    "    \"precision\": precision_score(actual_tags,\n",
    "                                 _predicted_tags,\n",
    "                                 average=\"macro\",\n",
    "                                 zero_division=0),\n",
    "    \"f1\": f1_score(actual_tags,\n",
    "                   _predicted_tags,\n",
    "                   average=\"macro\",\n",
    "                   zero_division=0)\n",
    "}\n",
    "\n",
    "pp(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
