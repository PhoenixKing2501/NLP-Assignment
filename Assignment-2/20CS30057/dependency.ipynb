{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from dependency_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = r\"./data/train.txt\"\n",
    "TEST_PATH = r\"./data/test.txt\"\n",
    "\n",
    "train_loader = Loader(TRAIN_PATH)\n",
    "test_loader = Loader(TEST_PATH)\n",
    "set_all(train_loader)\n",
    "print(f'{all_pos_tags = }')\n",
    "print(f'{all_dependency_labels = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given stack, buffer, and action sequence get the features for the current state of the parser.\n",
    "\n",
    "```text\n",
    "1. TOP:        top(S) token \n",
    "2. TOP.POS:    POS-Tag of top(S) \n",
    "3. TOP.DEP:    DEP-Tag for top(S) -> head(top(S)) or head(top(S)) <- top(S) € A (only if this has been seen so far)\n",
    "4: TOP.LDEP:   DEP-Tag for the left-most  w <- top(S) € A (only if this has been seen so far) \n",
    "5. TOP.RDEP:   DEP-Tag for the right-most top(S) -> w € A (only if this has been seen so far)\n",
    "6. FIRST:      first(B) token\n",
    "7. FIRST.POS:  POS-Tag of first(B)\n",
    "8. FIRST.LDEP: DEP-Tag for the left-most  w <- first(B) € A (only if this has been seen so far)\n",
    "9. LOOK.POS:   POS-Tag of first(B — {first(B)})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {\n",
    "    'LEFT-ARC': 0,\n",
    "    'RIGHT-ARC': 1,\n",
    "    'REDUCE': 2,\n",
    "    'SHIFT': 3,\n",
    "}\n",
    "actions_list = ['LEFT-ARC', 'RIGHT-ARC', 'REDUCE', 'SHIFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_1_hot(\n",
    "    item: Optional[str],\n",
    "    all_items: List[str]\n",
    "):\n",
    "    if item is None:\n",
    "        return np.zeros(len(all_items), dtype=np.int8)\n",
    "    return np.array([item == x for x in all_items], dtype=np.int8)\n",
    "\n",
    "\n",
    "def get_test_dep(\n",
    "    from_word: Word,\n",
    "    to_word: Word,\n",
    ") -> Optional[str]:\n",
    "    from_pos = from_word.pos_tag\n",
    "    to_pos = to_word.pos_tag\n",
    "\n",
    "    return test_pos_pair_dep_relations.get((from_pos, to_pos))\n",
    "\n",
    "\n",
    "def get_config(\n",
    "    stack: List[Word],\n",
    "    buffer: List[Word],\n",
    "    seen_list: List[Word],\n",
    "    arcs: List[Tuple[int, int]],\n",
    "    test: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the configuration of the stack and buffer\n",
    "\n",
    "    Args:\n",
    "        stack: The stack\n",
    "        buffer: The buffer\n",
    "        seen_list: All words in the sentence that have been seen so far [sentence - buffer]\n",
    "        action_sequence: List of tuples (from_idx, to_idx)\n",
    "\n",
    "    Returns:\n",
    "    (2 * V + 3 * P + 4 * R) dimensional vector\n",
    "        - where P is the set of all POS Tags.\n",
    "        - V is the set of all words.\n",
    "        - R is the set of all dependency relations.\n",
    "\n",
    "    All configurations are represented as 1 hot vectors concatenated together\n",
    "    \"\"\"\n",
    "\n",
    "    top = stack[-1] if stack else None\n",
    "    top_pos = top.pos_tag if top else None\n",
    "\n",
    "    first = buffer[0] if buffer else None\n",
    "    first_pos = first.pos_tag if first else None\n",
    "\n",
    "    look_pos = buffer[1].pos_tag if len(buffer) > 1 else None\n",
    "\n",
    "    top_dep: Optional[str] = None\n",
    "    top_ldep: Optional[str] = None\n",
    "    top_rdep: Optional[str] = None\n",
    "\n",
    "    first_ldep: Optional[str] = None\n",
    "\n",
    "    if top:\n",
    "        # set top_dep\n",
    "        for word in seen_list:\n",
    "            if (word.word_index, top.word_index) in arcs:\n",
    "                if test:\n",
    "                    top_dep = get_test_dep(word, top)\n",
    "                else:\n",
    "                    top_dep = top.dependency_label\n",
    "                break\n",
    "\n",
    "        # set top_ldep\n",
    "        for word in seen_list:\n",
    "            if (top.word_index, word.word_index) in arcs:\n",
    "                if test:\n",
    "                    top_ldep = get_test_dep(top, word)\n",
    "                else:\n",
    "                    top_ldep = word.dependency_label\n",
    "                break\n",
    "\n",
    "        # set top_rdep\n",
    "        for word in reversed(seen_list):\n",
    "            if (top.word_index, word.word_index) in arcs:\n",
    "                if test:\n",
    "                    top_rdep = get_test_dep(top, word)\n",
    "                else:\n",
    "                    top_rdep = word.dependency_label\n",
    "                break\n",
    "\n",
    "    if first:\n",
    "        # set first_ldep\n",
    "        for word in seen_list:\n",
    "            if (first.word_index, word.word_index) in arcs:\n",
    "                if test:\n",
    "                    first_ldep = get_test_dep(first, word)\n",
    "                else:\n",
    "                    first_ldep = word.dependency_label\n",
    "                break\n",
    "\n",
    "    top = top.norm_word if top else None\n",
    "    first = first.norm_word if first else None\n",
    "\n",
    "    return np.concatenate([\n",
    "        to_1_hot(top, all_words),\n",
    "        to_1_hot(top_pos, all_pos_tags),\n",
    "        to_1_hot(top_dep, all_dependency_labels),\n",
    "        to_1_hot(top_ldep, all_dependency_labels),\n",
    "        to_1_hot(top_rdep, all_dependency_labels),\n",
    "        to_1_hot(first, all_words),\n",
    "        to_1_hot(first_pos, all_pos_tags),\n",
    "        to_1_hot(first_ldep, all_dependency_labels),\n",
    "        to_1_hot(look_pos, all_pos_tags),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle for the parser is as follows:\n",
    "\n",
    "```text\n",
    "1. If first(B) -> top(S) € D and * -> top(S) not € A\n",
    "    then LEFT-ARC\n",
    "2. Else if top(S) -> first(B) € D\n",
    "    then RIGHT-ARC\n",
    "3. Else if * -> top(S) € A and there exists w € S, w != top(S) \n",
    "        such that (w -> first(B) € D or first(B) -> w € D)\n",
    "    then REDUCE\n",
    "4. Else SHIFT\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle(\n",
    "    stack: List[Word],                      # The stack S\n",
    "    buffer: List[Word],                     # The buffer B\n",
    "    arcs: List[Tuple[int, int]],            # The arcs A\n",
    "    gold_arcs: List[Tuple[int, int]],       # The full dependency tree D\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the next action to be taken by the parser\n",
    "    \"\"\"\n",
    "\n",
    "    top = stack[-1] if stack else None\n",
    "    first = buffer[0] if buffer else None\n",
    "\n",
    "    if top and first:\n",
    "        # LEFT-ARC\n",
    "        if ((first.word_index, top.word_index) in gold_arcs and\n",
    "            not any([to_idx == top.word_index\n",
    "                     for (from_idx, to_idx) in arcs])):\n",
    "            return actions['LEFT-ARC']\n",
    "\n",
    "        # RIGHT-ARC\n",
    "        if (top.word_index, first.word_index) in gold_arcs:\n",
    "            return actions['RIGHT-ARC']\n",
    "\n",
    "        # REDUCE\n",
    "        if (any([to_idx == top.word_index\n",
    "                for (from_idx, to_idx) in arcs]) and\n",
    "            any([(word.word_index, first.word_index) in gold_arcs or\n",
    "                 (first.word_index, word.word_index) in gold_arcs\n",
    "                 for word in stack[:-1]])):\n",
    "            return actions['REDUCE']\n",
    "    # END if top\n",
    "\n",
    "    return actions['SHIFT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn and inference the dependency parsing of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_weights(\n",
    "    train_loader: Loader,\n",
    "    test_loader: Loader,\n",
    "    epochs: int,\n",
    "):\n",
    "    weights = np.zeros((2 * len(all_words) +\n",
    "                        3 * len(all_pos_tags) +\n",
    "                        4 * len(all_dependency_labels), 4), dtype=np.int8)\n",
    "\n",
    "    max_uas_score = 0.0\n",
    "\n",
    "    for _ in trange(epochs, desc='Epochs'):\n",
    "        total_preds = 0\n",
    "        correct_preds = 0\n",
    "\n",
    "        for sentence in tqdm(train_loader, desc='Training Set'):\n",
    "            # train config\n",
    "            stack: List[Word] = []\n",
    "            buffer: List[Word] = sentence.copy().words\n",
    "            seen_list: List[Word] = []\n",
    "            arcs: List[Tuple[int, int]] = []\n",
    "\n",
    "            gold_arcs = [(word.head_index, word.word_index)\n",
    "                         for word in sentence]\n",
    "\n",
    "            while buffer:\n",
    "                config = get_config(stack=stack,\n",
    "                                    buffer=buffer,\n",
    "                                    seen_list=seen_list,\n",
    "                                    arcs=arcs)\n",
    "\n",
    "                oracle_action = oracle(stack=stack,\n",
    "                                       buffer=buffer,\n",
    "                                       arcs=arcs,\n",
    "                                       gold_arcs=gold_arcs)\n",
    "\n",
    "                if stack:\n",
    "                    # find the best action\n",
    "                    for i in range(weights.shape[1]):\n",
    "                        cost = config @ weights[:, i]\n",
    "\n",
    "                        if i == 0 or cost > max_cost:\n",
    "                            max_cost = cost.item()\n",
    "                            action = i\n",
    "                else:\n",
    "                    action = actions['SHIFT']\n",
    "\n",
    "                total_preds += 1\n",
    "                # update weights\n",
    "                if action != oracle_action:\n",
    "                    weights[:, action] -= config.T\n",
    "                    weights[:, oracle_action] += config.T\n",
    "                else:\n",
    "                    correct_preds += 1\n",
    "\n",
    "                # perform oracle action\n",
    "                if oracle_action == actions['LEFT-ARC']:\n",
    "                    arcs.append((buffer[0].word_index, stack[-1].word_index))\n",
    "                    stack.pop()\n",
    "                elif oracle_action == actions['RIGHT-ARC']:\n",
    "                    arcs.append((stack[-1].word_index, buffer[0].word_index))\n",
    "                    stack.append(buffer.pop(0))\n",
    "                    seen_list.append(stack[-1])\n",
    "                elif oracle_action == actions['REDUCE']:\n",
    "                    stack.pop()\n",
    "                elif oracle_action == actions['SHIFT']:\n",
    "                    stack.append(buffer.pop(0))\n",
    "                    seen_list.append(stack[-1])\n",
    "            # END while buffer\n",
    "        # END for sentence in tqdm(loader)\n",
    "\n",
    "        print('Training Set Results:')\n",
    "        print(f'{correct_preds = }')\n",
    "        print(f'{total_preds = }')\n",
    "        print(f'{correct_preds / total_preds = }\\n')\n",
    "\n",
    "        _, _, uas = get_test_preds(test_loader, weights)\n",
    "\n",
    "        if uas > max_uas_score:\n",
    "            max_uas_score = uas\n",
    "            np.save('dependency_model_on.npy', weights)\n",
    "            print(f'{max_uas_score = }\\n\\n')\n",
    "\n",
    "    # END for _ in trange(epochs)\n",
    "\n",
    "    return weights\n",
    "# END learn_weights\n",
    "\n",
    "\n",
    "def get_test_preds(\n",
    "    loader: Loader,\n",
    "    weights: np.ndarray,\n",
    "    save: bool = False,\n",
    "):\n",
    "    correct_arcs = 0\n",
    "    total_arcs = 0\n",
    "\n",
    "    for sentence in tqdm(loader, desc='Testing Set'):\n",
    "        stack: List[Word] = []\n",
    "        buffer: List[Word] = sentence.copy().words\n",
    "        seen_list: List[Word] = []\n",
    "        arcs: List[Tuple[int, int]] = []\n",
    "\n",
    "        gold_arcs = [(word.head_index, word.word_index)\n",
    "                     for word in sentence]\n",
    "\n",
    "        while buffer:\n",
    "            config = get_config(stack=stack,\n",
    "                                buffer=buffer,\n",
    "                                seen_list=seen_list,\n",
    "                                arcs=arcs,\n",
    "                                test=True,)\n",
    "\n",
    "            action: int\n",
    "            max_cost: int\n",
    "\n",
    "            # find the best action\n",
    "            for i in range(weights.shape[1]):\n",
    "                cost = config @ weights[:, i]\n",
    "\n",
    "                if i == 0 or cost > max_cost:\n",
    "                    max_cost = cost.item()\n",
    "                    action = i\n",
    "\n",
    "            # perform action\n",
    "            if stack:\n",
    "                # if head(top(S)) has been seen then REDUCE\n",
    "                # else LEFT-ARC\n",
    "                if action == actions['LEFT-ARC']:\n",
    "                    if not any([to_idx == stack[-1].word_index\n",
    "                                for (from_idx, to_idx) in arcs]):\n",
    "                        arcs.append((buffer[0].word_index,\n",
    "                                     stack[-1].word_index))\n",
    "                    stack.pop()\n",
    "                elif action == actions['RIGHT-ARC']:\n",
    "                    arcs.append((stack[-1].word_index,\n",
    "                                 buffer[0].word_index))\n",
    "                    stack.append(buffer.pop(0))\n",
    "                    seen_list.append(stack[-1])\n",
    "\n",
    "                # if head(top(S)) has been seen then REDUCE\n",
    "                # else LEFT-ARC\n",
    "                elif action == actions['REDUCE']:\n",
    "                    if any([to_idx == stack[-1].word_index\n",
    "                            for (from_idx, to_idx) in arcs]):\n",
    "                        arcs.append((buffer[0].word_index,\n",
    "                                     stack[-1].word_index))\n",
    "                    stack.pop()\n",
    "                else:\n",
    "                    stack.append(buffer.pop(0))\n",
    "                    seen_list.append(stack[-1])\n",
    "            else:\n",
    "                stack.append(buffer.pop(0))\n",
    "                seen_list.append(stack[-1])\n",
    "        # END while buffer\n",
    "\n",
    "        total_arcs += len(gold_arcs)\n",
    "        correct_arcs += sum([arc in arcs for arc in gold_arcs])\n",
    "        \n",
    "        if save:\n",
    "            with open('dependency_predictions_on.tsv',\"a\",\n",
    "                      encoding='utf-8') as f:\n",
    "                for arc in arcs:\n",
    "                    f.write(f'{sentence.sent_id}\\t')\n",
    "                    f.write(f'{arc[1]+1}\\t')\n",
    "                    f.write(f'{sentence[arc[1]].word}\\t')\n",
    "                    f.write(f'{arc[0]+1}\\n')\n",
    "    # END for sentence in tqdm(loader)\n",
    "\n",
    "    print('Testing Set Results:')\n",
    "    print(f'{correct_arcs = }')\n",
    "    print(f'{total_arcs = }')\n",
    "    print(f'{correct_arcs / total_arcs = }\\n')\n",
    "\n",
    "    return correct_arcs, total_arcs, correct_arcs / total_arcs\n",
    "# END get_test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using the given training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "weights = learn_weights(train_loader,\n",
    "                        test_loader,\n",
    "                        EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "weights = np.load('dependency_model_on.npy')\n",
    "\n",
    "correct_preds, total_preds, uas_score = get_test_preds(\n",
    "    loader=test_loader,\n",
    "    weights=weights,\n",
    "    save=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
